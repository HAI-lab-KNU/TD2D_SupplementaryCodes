{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c508e33-c12c-44f6-a775-91c3e94e0fdc",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d4f61-9e6b-4209-b01e-dd3bdc9762f2",
   "metadata": {},
   "source": [
    "## Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "545f22dc-bd8a-4f95-b485-78c440ea0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH_DATA='./TD2D'\n",
    "sensor_data = ['PPG.csv', 'ECG.csv', 'EDA.csv', 'HR.csv', 'diameter.csv', 'fixations.csv', 'gazePositions.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5b8d2-9601-4f5b-b3ef-a6a398affe9a",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24b082f9-d076-4df8-99bf-87534edba002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9737f87c-9ec4-464c-a3b7-d35a8a805c06",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1033e779-c990-42fa-bade-7d9cfbb6e5a1",
   "metadata": {},
   "source": [
    "## preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88db933e-73ef-44c6-97e1-85a6b75df6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.DataFrame()\n",
    "\n",
    "with os.scandir(PATH_DATA) as root:\n",
    "    for pid in root: #D1~D50\n",
    "        driver = os.path.basename(pid)[1:]\n",
    "\n",
    "        if driver.__contains__('idea'):\n",
    "            continue\n",
    "        elif driver.__contains__('xlsx'):\n",
    "            continue\n",
    "        elif driver.__contains__('DS_Store'):\n",
    "            continue\n",
    "\n",
    "        if os.path.isdir(pid):\n",
    "            with os.scandir(pid) as p:\n",
    "                label_driver = pd.DataFrame()\n",
    "                for task in p:  # e.g., audiobook listening\n",
    "                    if os.path.isdir(task):\n",
    "                        with os.scandir(task) as t:\n",
    "                            secondary_task = os.path.basename(task)\n",
    "                            #참조할 cases information csv file\n",
    "                            cases_information = pd.read_csv(os.path.abspath(task)+'/takeoverScenarioMeasurements.csv', sep=',')\n",
    "                            #label\n",
    "                            label_task = pd.DataFrame([[driver, secondary_task]])\n",
    "                            label_task = pd.concat([label_task, cases_information[['takeoverResult','reactionTime', 'NASA-TLX']]], axis=1, ignore_index=True)\n",
    "                            label_driver = pd.concat([label_driver, label_task], axis=0, ignore_index=True)\n",
    "                label = pd.concat([label, label_driver], axis=0, ignore_index=True)\n",
    "label.columns = ['driver', 'secondaryTask', 'takeoverResult', 'reactionTime', 'NASA_TLX']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c526648-18c0-4463-85fc-606c3eeef6e7",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a636f5f-d4a6-4286-b95f-a7eea43f7d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secondary Task                  Success Ratio (%)   Reaction Time (ms)        Perceived Workload\n",
      "                                                    Mean       SD             Mean     SD\n",
      "<Without secondary task>\n",
      "baseline                        86.0                880.98     201.80         29.60     22.99\n",
      "<Auditory tasks>\n",
      "0back                           82.0                883.06     235.94         33.37     21.18\n",
      "1back                           68.0                955.50     253.31         51.73     20.09\n",
      "2back                           72.0                978.62     209.32         66.91     16.44\n",
      "audiobook listening             76.0                905.26     213.36         38.28     18.95\n",
      "auditory texting                68.0                1016.90     264.46         45.81     20.92\n",
      "auditory gaming                 74.0                980.86     281.65         41.99     21.83\n",
      "<Visual tasks>\n",
      "ebook reading                   36.0                1294.30     483.59         71.25     15.06\n",
      "texting                         40.0                1352.58     433.19         60.47     18.71\n",
      "gaming                          38.0                1312.24     452.80         62.45     20.58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define secondary task categories\n",
    "def categorize_task(task):\n",
    "    if task == 'baseline':\n",
    "        return '<Without secondary task>'\n",
    "    elif task in ['0back', '1back', '2back', 'audiobook listening', 'auditory gaming', 'auditory texting']:\n",
    "        return '<Auditory tasks>'\n",
    "    elif task in ['ebook reading', 'texting', 'gaming']:\n",
    "        return '<Visual tasks>'\n",
    "    return '<Unknown>'\n",
    "\n",
    "label['Category'] = label['secondaryTask'].apply(categorize_task)\n",
    "\n",
    "# Calculate success ratio\n",
    "success_ratios = label.groupby('secondaryTask').apply(lambda x: (x['takeoverResult'] == 'Success').mean() * 100)\n",
    "\n",
    "# Calculate means and standard deviations for reaction time and workload\n",
    "stats = label.groupby('secondaryTask').agg(\n",
    "    reactionTime_Mean=('reactionTime', 'mean'),\n",
    "    reactionTime_SD=('reactionTime', 'std'),\n",
    "    workload_Mean=('NASA_TLX', 'mean'),\n",
    "    workload_SD=('NASA_TLX', 'std')\n",
    ")\n",
    "\n",
    "# Merge success ratios\n",
    "stats = stats.join(success_ratios.rename('successRatio'))\n",
    "\n",
    "# Add category information to the grouped statistics\n",
    "stats = stats.reset_index().merge(label[['secondaryTask', 'Category']].drop_duplicates(), on='secondaryTask').set_index('secondaryTask')\n",
    "\n",
    "# Reorder columns\n",
    "stats = stats[['Category', 'successRatio', 'reactionTime_Mean', 'reactionTime_SD', 'workload_Mean', 'workload_SD']]\n",
    "\n",
    "# Set custom order for secondary tasks\n",
    "task_order = [\n",
    "    'baseline', '0back', '1back', '2back', 'audiobook listening',\n",
    "    'auditory texting', 'auditory gaming', 'ebook reading', 'texting', 'gaming'\n",
    "]\n",
    "stats.index = pd.CategoricalIndex(stats.index, categories=task_order, ordered=True)\n",
    "stats = stats.sort_index()\n",
    "\n",
    "# Define a function to format the output like the provided table\n",
    "def format_stats_table_with_headers(label):\n",
    "    table = \"Secondary Task                  Success Ratio (%)   Reaction Time (ms)        Perceived Workload\\n\"\n",
    "    table += \"                                                    Mean       SD             Mean     SD\\n\"\n",
    "    current_category = None\n",
    "    for index, row in label.iterrows():\n",
    "        if row['Category'] != current_category:\n",
    "            current_category = row['Category']\n",
    "            table += f\"{current_category}\\n\"\n",
    "        table += f\"{index:<30} {row['successRatio']:>5.1f}                {row['reactionTime_Mean']:>6.2f}     {row['reactionTime_SD']:>6.2f}        {row['workload_Mean']:>6.2f}    {row['workload_SD']:>6.2f}\\n\"\n",
    "    return table\n",
    "\n",
    "# Format the table and print it\n",
    "formatted_stats_table = format_stats_table_with_headers(stats)\n",
    "print(formatted_stats_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28748b61-7333-4c07-ae34-b1d327c4fede",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a2490d-7c12-4db5-ba86-f0ad79506793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation is:\n",
      "                reactionTime  takeoverResult  NASA_TLX\n",
      "reactionTime        1.000000       -0.598944  0.285091\n",
      "takeoverResult     -0.598944        1.000000 -0.326345\n",
      "NASA_TLX            0.285091       -0.326345  1.000000 \n",
      "\n",
      "The p-value is:\n",
      "                reactionTime  takeoverResult      NASA_TLX\n",
      "reactionTime    0.000000e+00    5.329990e-50  8.351804e-11\n",
      "takeoverResult  5.329990e-50    0.000000e+00  7.159309e-14\n",
      "NASA_TLX        8.351804e-11    7.159309e-14  0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "x = label.copy()\n",
    "x.takeoverResult = x.takeoverResult.apply(lambda x: 1 if x == 'Success' else 0)\n",
    "x = x[['reactionTime', 'takeoverResult', 'NASA_TLX']]\n",
    "\n",
    "corr_x = x.corr()\n",
    "p_x = x.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(len(x.columns))\n",
    "\n",
    "print(\"The correlation is:\")\n",
    "print(corr_x, \"\\n\")\n",
    "print(\"The p-value is:\")\n",
    "print(p_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843bfa98-9b38-454a-9f29-5fe2ec88cb3a",
   "metadata": {},
   "source": [
    "# Preprocessing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "547ca954-34f4-46d1-b3cd-3f164afe5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract(data: pd.DataFrame): # return features as a dataframe\n",
    "    features = pd.DataFrame([[data.min(), data.max(), data.mean(), data.skew(), data.kurtosis()]])\n",
    "    return features\n",
    "FEATURE_NUMBER = 5 # min, max, mean, skewness, kurtosis\n",
    "VALUE_NUMBER = 13 # sum of values\n",
    "DATA_DURATION = 10000 # 10 s\n",
    "\n",
    "feature = pd.DataFrame()\n",
    "label = pd.DataFrame()\n",
    "\n",
    "with os.scandir(PATH_DATA) as root:\n",
    "    for pid in root: #D1~D50\n",
    "        driver = os.path.basename(pid)[1:]\n",
    "\n",
    "        if driver.__contains__('idea'):\n",
    "            continue\n",
    "        elif driver.__contains__('xlsx'):\n",
    "            continue\n",
    "        elif driver.__contains__('DS_Store'):\n",
    "            continue\n",
    "\n",
    "        if os.path.isdir(pid):\n",
    "            with os.scandir(pid) as p:\n",
    "                feature_driver = pd.DataFrame()\n",
    "                label_driver = pd.DataFrame()\n",
    "                for task in p:  # 01_task_name\n",
    "                    if os.path.isdir(task):\n",
    "                        with os.scandir(task) as t:\n",
    "                            # takeoverScenarioMeasurements csv file\n",
    "                            takeoverScenarioMeasurements = pd.read_csv(os.path.abspath(task)+'/takeoverScenarioMeasurements.csv', sep=',')\n",
    "                            critical_event_occurrence_time = takeoverScenarioMeasurements.criticalEventOccurrenceTime.values[0]\n",
    "\n",
    "                            # Dataframe for feature extract\n",
    "                            feature_task = pd.DataFrame([driver], columns=['driver'])\n",
    "\n",
    "                            # Label\n",
    "                            label_task = pd.DataFrame([driver], columns=['driver'])\n",
    "                            label_task = pd.concat([label_task, takeoverScenarioMeasurements[['takeoverResult', 'reactionTime', 'NASA-TLX']]], axis=1, ignore_index=True)\n",
    "\n",
    "                            csvs = ['fixations', 'gazePositions', 'diameter', 'HR', 'PPG', 'ECG', 'EDA']\n",
    "                            for csv in t:   # e.g., ECG.csv\n",
    "                                f = os.path.basename(csv)\n",
    "                                if sensor_data.__contains__(f):\n",
    "                                    file = pd.read_csv(csv, sep=',')\n",
    "                                    \n",
    "                                    #selecting 10 seconds before critical event occurrence \n",
    "                                    idx_not_selected = file[(file['timestamp'] < (float(critical_event_occurrence_time) - DATA_DURATION)) | (file['timestamp'] > float(critical_event_occurrence_time))].index\n",
    "                                    file = file.drop(idx_not_selected)\n",
    "\n",
    "                                    if file.empty:\n",
    "                                        continue\n",
    "\n",
    "                                    # get feature\n",
    "                                    if f == 'fixations.csv':\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 1])], axis=1, ignore_index=True)\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 2])], axis=1, ignore_index=True)\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 3])], axis=1, ignore_index=True)\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 4])], axis=1, ignore_index=True)\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 5])], axis=1, ignore_index=True)\n",
    "                                    elif f == 'gazePositions.csv':\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 1])], axis=1, ignore_index=True)\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 2])], axis=1, ignore_index=True)\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 3])], axis=1, ignore_index=True)\n",
    "                                    elif f == 'diameter.csv':\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 1])], axis=1, ignore_index=True)\n",
    "                                    elif f == 'HR.csv':\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 1])], axis=1, ignore_index=True)\n",
    "                                    elif f == 'PPG.csv':\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 1])], axis=1, ignore_index=True)\n",
    "                                    elif f == 'ECG.csv':\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 1])], axis=1, ignore_index=True)\n",
    "                                    elif f == 'EDA.csv':\n",
    "                                        feature_task = pd.concat([feature_task, _extract(file.iloc[:, 1])], axis=1, ignore_index=True)\n",
    "                            feature_driver = pd.concat([feature_driver, feature_task], axis=0, ignore_index=True)\n",
    "                            label_driver = pd.concat([label_driver, label_task], axis=0, ignore_index=True)\n",
    "                if (not feature_driver.isna().values.any()) & (len(feature_driver.columns) == 1+VALUE_NUMBER*FEATURE_NUMBER):\n",
    "                    feature = pd.concat([feature, feature_driver], axis=0, ignore_index=True)\n",
    "                    label = pd.concat([label, label_driver], axis=0, ignore_index=True)\n",
    "label.columns = ['driver', 'takeoverResult', 'reactionTime', 'NASA-TLX']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415380e5-b0cf-4c1e-a128-d9afc1c9d0db",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34579d9f-3361-4dfe-a408-a7ef08f23ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "MAX_DEPTH = None\n",
    "LEARNING_RATE = 0.2\n",
    "\n",
    "ESTIMATOR_DUMMY_CLF = DummyClassifier(strategy='prior')\n",
    "ESTIMATOR_RF_CLF = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_XGB_CLF = XGBClassifier(random_state=RANDOM_STATE)\n",
    "ESTIMATOR_LGBM_CLF = LGBMClassifier(random_state=RANDOM_STATE, verbose=-1, importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2898ab4a-ac43-414c-9fda-583657f357d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(feature.iloc[:, 1:])\n",
    "y_tr = np.array(label.iloc[:, [1]]).ravel()\n",
    "y_tr = np.where(y_tr == \"Success\", 1, 0)\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "groups = np.array(feature.iloc[:,[0]]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e65cdf2-847a-44cc-9ae7-c7f6048324bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=RANDOM_STATE)\n",
    "\n",
    "N_FEATURES = 20\n",
    "\n",
    "DUMMY_accuracies = []\n",
    "DUMMY_f1_scores = []\n",
    "RF_accuracies = []\n",
    "RF_f1_scores = []\n",
    "XGB_accuracies = []\n",
    "XGB_f1_scores = []\n",
    "LGBM_accuracies = []\n",
    "LGBM_f1_scores = []\n",
    "RF_rankings = [0] * 65\n",
    "XGB_rankings = [0] * 65\n",
    "LGBM_rankings = [0] * 65\n",
    "\n",
    "#classifier\n",
    "for train_idx, test_idx in logo.split(X, y_tr, groups):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_clf_train, y_clf_test = y_tr[train_idx], y_tr[test_idx]\n",
    "    \n",
    "    #oversample\n",
    "    X_train, y_clf_train = sm.fit_resample(X_train, y_clf_train)\n",
    "\n",
    "    #Recursive Feature Elimination\n",
    "    \n",
    "    #RF model building\n",
    "    selector = RFE(ESTIMATOR_RF_CLF, n_features_to_select=N_FEATURES, step=5)\n",
    "    selector = selector.fit(X_train, y_clf_train)\n",
    "    y_rf_clf_pred = selector.predict(X_test)\n",
    "    \n",
    "    RF_accuracy = selector.score(X_test, y_clf_test)\n",
    "    RF_f1_score = f1_score(y_clf_test, y_rf_clf_pred, average='macro')\n",
    "    RF_accuracies.append(RF_accuracy)\n",
    "    RF_f1_scores.append(RF_f1_score)\n",
    "    RF_rankings = RF_rankings + selector.ranking_\n",
    "    \n",
    "    #XGB model building\n",
    "    selector = RFE(ESTIMATOR_XGB_CLF, n_features_to_select=N_FEATURES, step=5)\n",
    "    selector = selector.fit(X_train, y_clf_train)\n",
    "    y_xgb_clf_pred = selector.predict(X_test)\n",
    "    \n",
    "    XGB_accuracy = selector.score(X_test, y_clf_test)\n",
    "    XGB_f1_score = f1_score(y_clf_test, y_xgb_clf_pred, average='macro')\n",
    "    XGB_accuracies.append(XGB_accuracy)\n",
    "    XGB_f1_scores.append(XGB_f1_score)\n",
    "    XGB_rankings = XGB_rankings + selector.ranking_\n",
    "\n",
    "    #LGBM model building\n",
    "    selector = RFE(ESTIMATOR_LGBM_CLF, n_features_to_select=N_FEATURES, step=5)\n",
    "    selector = selector.fit(X_train, y_clf_train)\n",
    "    y_lgbm_clf_pred = selector.predict(X_test)\n",
    "    \n",
    "    LGBM_accuracy = selector.score(X_test, y_clf_test)\n",
    "    LGBM_f1_score = f1_score(y_clf_test, y_lgbm_clf_pred, average='macro')\n",
    "    LGBM_accuracies.append(LGBM_accuracy)\n",
    "    LGBM_f1_scores.append(LGBM_f1_score)\n",
    "    LGBM_rankings = LGBM_rankings + selector.ranking_\n",
    "\n",
    "    #DUMMY model building\n",
    "    ESTIMATOR_DUMMY_CLF.fit(X_train, y_clf_train.ravel())\n",
    "    y_dummy_clf_pred = ESTIMATOR_DUMMY_CLF.predict(X_test)\n",
    "    \n",
    "    #DUMMY model evaluation\n",
    "    DUMMY_accuracy = ESTIMATOR_DUMMY_CLF.score(X_test, y_clf_test)\n",
    "    DUMMY_f1_score = f1_score(y_clf_test, y_dummy_clf_pred, average='macro')\n",
    "\n",
    "    DUMMY_accuracies.append(DUMMY_accuracy)\n",
    "    DUMMY_f1_scores.append(DUMMY_f1_score)\n",
    "\n",
    "# Calculate averages and standard deviations\n",
    "RF_avg_accuracy = np.mean(RF_accuracies)\n",
    "RF_std_accuracy = np.std(RF_accuracies)\n",
    "RF_avg_f1_score = np.mean(RF_f1_scores)\n",
    "RF_std_f1 = np.std(RF_f1_scores)\n",
    "\n",
    "XGB_avg_accuracy = np.mean(XGB_accuracies)\n",
    "XGB_std_accuracy = np.std(XGB_accuracies)\n",
    "XGB_avg_f1_score = np.mean(XGB_f1_scores)\n",
    "XGB_std_f1 = np.std(XGB_f1_scores)\n",
    "\n",
    "LGBM_avg_accuracy = np.mean(LGBM_accuracies)\n",
    "LGBM_std_accuracy = np.std(LGBM_accuracies)\n",
    "LGBM_avg_f1_score = np.mean(LGBM_f1_scores)\n",
    "LGBM_std_f1 = np.std(LGBM_f1_scores)\n",
    "\n",
    "DUMMY_avg_accuracy = np.mean(DUMMY_accuracies)\n",
    "DUMMY_std_accuracy = np.std(DUMMY_accuracies)\n",
    "DUMMY_avg_f1_score = np.mean(DUMMY_f1_scores)\n",
    "DUMMY_std_f1 = np.std(DUMMY_f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18a530ec-ae4c-48c5-96a6-9c20424d64f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Avg. F1 (SD)  Avg. Accuracy (SD) \n",
      "Baseline              0.22 (0.14)   0.33 (0.25) \n",
      "Random Forest         0.61 (0.18)   0.71 (0.14) \n",
      "XGBoost               0.56 (0.18)   0.68 (0.14) \n",
      "LightGBM              0.57 (0.21)   0.69 (0.18) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to format the results into a table\n",
    "def format_results_table(results):\n",
    "    table = (\n",
    "        \"                      Avg. F1 (SD)  Avg. Accuracy (SD) \\n\"\n",
    "    )\n",
    "    for model, (avg_f1, std_f1, avg_acc, std_acc) in results.items():\n",
    "        table += f\"{model:<20}  {avg_f1:.2f} ({std_f1:.2f})   {avg_acc:.2f} ({std_acc:.2f}) \\n\"\n",
    "    return table\n",
    "\n",
    "# Results dictionary\n",
    "results = {\n",
    "    \"Baseline\": (DUMMY_avg_f1_score, DUMMY_std_f1, DUMMY_avg_accuracy, DUMMY_std_accuracy),\n",
    "    \"Random Forest\": (RF_avg_f1_score, RF_std_f1, RF_avg_accuracy, RF_std_accuracy),\n",
    "    \"XGBoost\": (XGB_avg_f1_score, XGB_std_f1, XGB_avg_accuracy, XGB_std_accuracy),\n",
    "    \"LightGBM\": (LGBM_avg_f1_score, LGBM_std_f1, LGBM_avg_accuracy, LGBM_std_accuracy),\n",
    "}\n",
    "\n",
    "# Format and print the results table\n",
    "formatted_results_table = format_results_table(results)\n",
    "print(formatted_results_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1632b-8632-45e3-b2bd-d5e884416f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
